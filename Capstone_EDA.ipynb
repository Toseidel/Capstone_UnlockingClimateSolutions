{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title image](figures/cdp-logo_16_9_capstone.png)\n",
    "\n",
    "# 1. Business Understanding\n",
    "Ask relevant questions and define objectives for the problem that needs to be tackled\n",
    "\n",
    "## 1.1 About CDP\n",
    "CDP is a global non-profit that drives companies and governments to reduce their greenhouse gas emissions, safeguard water resources, and protect forests. Each year, CDP takes the information supplied in its annual reporting process and scores companies and cities based on their journey through disclosure and towards environmental leadership.\n",
    "\n",
    "CDP houses the world’s largest, most comprehensive dataset on environmental action. As the data grows to include thousands more companies and cities each year, there is increasing potential for the data to be utilized in impactful ways. Because of this potential, CDP is excited to launch an analytics challenge for the Kaggle community. Data scientists will scour environmental information provided to CDP by disclosing companies and cities, searching for solutions to our most pressing problems related to climate change, water security, deforestation, and social inequity.\n",
    "\n",
    "## 1.2 Questions\n",
    "- How do you help cities adapt to a rapidly changing climate amidst a global pandemic, but do it in a way that is socially equitable?\n",
    "- What are the projects that can be invested in that will help pull cities out of a recession, mitigate climate issues, but not perpetuate racial/social inequities?\n",
    "- What are the practical and actionable points where city and corporate ambition join, i.e. where do cities have problems that corporations affected by those problems could solve, and vice versa?\n",
    "- How can we measure the intersection between environmental risks and social equity, as a contributor to resiliency?\n",
    "\n",
    "## 1.3 Problem Statement\n",
    "Develop a methodology for calculating key performance indicators (KPIs) that relate to the environmental and social issues that are discussed in the CDP survey data. Leverage external data sources and thoroughly discuss the intersection between environmental issues and social issues. Mine information to create automated insight generation demonstrating whether city and corporate ambitions take these factors into account.\n",
    "\n",
    "## 1.4 Project Goal\n",
    "We develop relevant KPIs to help cities and companies optimise and communicate their own climate protection strategy. The aim is to identify and visualise activities and projects that strike the best possible balance between climate protection and aspects of social justice. We rely on an extensive Exploratory Data Analysis of the CDP-Surveys with more than 1.5 million annual observations. A main focus of our work is the generation of new features and the explanation of the insights gained from them. For the interpretation and classification of unstructured free text responses we use NLP / Sentiment Analysis techniques. We use cluster algorithms to show the connections between social justice and environmental protection, as well as between the interests of the public sector and private companies.\n",
    "\n",
    "## 1.5 Evaluations\n",
    "**Accuracy/Completeness**\n",
    "- Did the author develop one or more key performance indicators (KPIs)?\n",
    "- Did the author provide a way of assessing the performance and accuracy of their solution?\n",
    "- Are the KPIs useful for discussing relationships between social issues and environmental issues and demonstrating whether city and corporate ambitions take these factors into account?\n",
    "- Do the KPIs accurately reflect the underlying data?  \n",
    "\n",
    "**Communication**\n",
    "- Does the notebook have a compelling and coherent narrative?\n",
    "- Does the notebook contain data visualizations that help to communicate the author’s main points?\n",
    "- Did the author include a thorough discussion on the intersection between environmental issues and social issues?\n",
    "- Was there discussion of automated insight generation, demonstrating whether city and corporate ambitions take these factors into account?  \n",
    "\n",
    "**Documentation**\n",
    "- Is the code documented in a way that makes it easy to understand and reproduce?\n",
    "- Were all external sources of data made public and cited appropriately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Details\n",
    "\n",
    "## 2.1 Origin of the dataset\n",
    "Explain...\n",
    "\n",
    "## 2.2 Directory and File-structure\n",
    "![title image](figures/CDP_dataset.png)\n",
    "\n",
    "## 2.3 Feature Glossary\n",
    "\n",
    "### 2.3.1 Cities Disclosing (cid)\n",
    "- **year_reported_to_cdp:** Cities Disclosure cycle survey year  \n",
    "- **account_number:** The unique identifier given to every city organisation that receives a request to complete a CDP questionnaire  \n",
    "- **organization:** Name of the City organisation disclosing  \n",
    "- **city:** Name of the City the city organisation is disclosing on behalf of  \n",
    "- **country:** Country of city  \n",
    "- **cdp_region:** CDP operation region City is located within  \n",
    "- **reporting_authority:** \"CDP collects information on behalf of a number of additional initiatives. Other than CDP Cities, organisations can indicate the additional initiatve they are have answered questions for \",\"C40,CDP Cities,ICLEI - Local Governments for Sustainability\",\"Includes Global Covenant of Mayors for Climate and Energy, ICLEI Green Climate Cities, ICLEI Ecomobility / Ecologistics, C40 Cities Climate Leadership Group\"  \n",
    "- **access:** Cities can submit CDP response in public status or in non public status. Non public responses can only be shared within CDP and between signatory partners. Public responses can be shared beyond CDP City organisations,public  \n",
    "- **first_time_discloser:** Is the City disclosing for the first time to CDP  \n",
    "- **population:** Citiy population estimate  \n",
    "- **population_year:** City population estimate year  \n",
    "- **city_location:** \"Citty location cordinates by longitude, latitide\"  \n",
    "- **last_update:** Resonse record last update  \n",
    "\n",
    "### 2.3.2 Cities Responses (cir)\n",
    "- **questionnaire:** Questionnaire and questionnaire year the company's response relates to\n",
    "- **year_reported_to_cdp:** Cities Disclosure cycle survey year  \n",
    "- **account_number:** The unique identifier given to every city organisation that receives a request to complete a CDP questionnaire  \n",
    "- **organization:** Name of the City organisation disclosing  \n",
    "- **country:** Country of city    \n",
    "- **cdp_region:** CDP operation region City is located within  \n",
    "- **parent_section:** Module ('Parent Section') of the CDP questionnaire the question belongs to (e.g. Emissions Reduction)  \n",
    "- **section:** Section of the CDP questionnaire the question belongs to (e.g.Mitigation Actions)  \n",
    "- **question_number:** Question number of response (e.g. 5.4) \n",
    "- **question_name:** Describes the anticipated outcomes of the most impactful mitigation actions your city is currently undertaking; the total cost of the action and how much is being funded by the local government \n",
    "- **column_number:** Column number of matrix set (Table) or matrix dynamic (Add Rows Table) column in question  reponse table  \n",
    "- **column_name:** Column name of matrix set (Table) or matrix dynamic (Add Rows Table) column in question  reponse table,Co-benefit area \n",
    "- **row_number:** \"Row number of matrix set (Table) or matrix dynamic (Add Rows Table) row in question  reponse table. If originally submitted in a table format, this will indicate the number of rows of response data has been entered in response to a question. \",  \n",
    "- **row_name:** Row name of matrix set (Table) or matrix dynamic (Add Rows Table) row in question  reponse table. Description of data type for RowNumber where applicable, Population that is food insecure)  \n",
    "- **response_answer:** Question response submitted by company,Greening the economy,\"Can range from string, integar and double data types. Question not applicable = This question was not presented to the company to be answered due to conditional logic in the questionnaire. NA = The company was presented with this question but did not respond\" \n",
    "\n",
    "### 2.3.3 Corporations Disclosing (cod)\n",
    "- **account_number:** The unique identifier given to every company that receives a request to complete a CDP questionnaire.\n",
    "- **organization:** Name of the company disclosing.\n",
    "- **survey_year:** Disclosure cycle survey year. (E.g. survey year 2020 ran from March 2020 - September 2020)\n",
    "- **country:** Country in which the company is incorporated or legally registered.\n",
    "- **region:** CDP operating region in which the company is incorporated or legally registered.\n",
    "- **invitation_status:** CDP invites companies to disclose to the Investor request. If they choose to disclose, they will appear as \"\"submitted\"\".\n",
    "- **public:** Companies can submit CDP response in public status or in non public status. Non public responses can only be shared within CDP. Public responses can be shared beyond CDP investor signatories.\n",
    "- **samples:** CDP uses Market Cap from major indices and other environmental factors to help determine who should be requested to respond. Company's are distributed among sample groups to group similar organisations for targetted  invitations to disclose etc.(Continuity, Companies that disclosed the previous year are automatically requested to disclose the following year.)\n",
    "- **response_received_date:** DateTime company response was first received within CDP response systems,2018-08-15T00:00:00Z\n",
    "- **minimum_tier:** Indicates if the highest questionnaire tier a company has responded to. Company's can choose or are requested to submit to a shorter 'Minimum tier' questionnaire or a more in-depth 'Full tier' questionnaire with extended questions. Certain questions are therefore only available in the Full questionnaire.\n",
    "- **selected_tier:** Indicates if the questionnaire tier a company has responded to. Company's can choose or are requested to submit to a shorter 'Minimum tier' questionnaire or a more in-depth 'Full tier' questionnaire with extended questions.  Certain questions are therefore only available in the Full questionnaire. \n",
    "- **questionnaire:** Questionnaire and questionnaire year the company's response relates to.\n",
    "- **theme:** Questionnaire Theme the company's response relates to.\n",
    "- **authority_types:** Company's can be requested to respond to the CDP questionnaire by either/both CDP investor signatories and CDP Supply Chain members as suppliers that constititute their supply chain operations.\n",
    "- **activities:** CDP Activity Classification System categorizes companies according to their different business streams, revenue and impact on the environment. All  company's potential  business activities based on revenue, within the CDP Activity Classification System (e.g. Aluminium refining, Aluminum, Engines & motors, Fabricated metal components, Other vehicle equipment & systems). \n",
    "- **sectors:** CDP Activity Classification System categorizes companies according to their different business streams, revenue and impact on the environment. All  company's  potential business sectors based on revenue, within the CDP Activity Classification System (e.g. Metal products manufacturing, Metal smelting, refining & forming, Powered machinery). \n",
    "- **industries:** CDP Activity Classification System categorizes companies according to their different business streams, revenue and impact on the environment.  All  company's  potential business industries based on revenue, within the CDP Activity Classification System (e.g. Manufacturing, Materials).\n",
    "- **primary_activity:** CDP Activity Classification System categorizes companies according to their different business streams, revenue and impact on the environment. A company's primary business activity based on revenue; the most specific classification of three tiers in the CDP Activity Classification System.\n",
    "- **primary_sector:** CDP Activity Classification System categorizes companies according to their different business streams, revenue and impact on the environment. A company's primary business sector based on revenue; the second most specific classification of three tiers in the CDP Activity Classification System (e.g. Powered machinery)\n",
    "- **primary_industry:** CDP Activity Classification System categorizes companies according to their different business streams, revenue and impact on the environment. A company's primary industry based on revenue; the broadest classification of three tiers in the CDP Activity Classification System.\n",
    "- **primary_questionnaire_sector:** Describes the sector-specific questionnaire that was provided to the company based on their largest activity, if this version of the general questoinnaire was available.\n",
    "- **primary_ticker:** Financial  Market identifier for company.\n",
    "- **tickers:** Market identifiers (if more than one).\n",
    "\n",
    "### 2.3.4 Corporations Responses (cor)\n",
    "- **account_number:** The unique identifier given to every company that receives a request to complete a CDP questionnaire.\n",
    "- **organization:** Name of the company disclosing.\n",
    "- **survey_year:** Disclosure cycle survey year. \n",
    "- **response_received_date:** DateTime company response was first received within CDP response systems.\n",
    "- **accounting_period_to:** Accounting year end for the survey responses provided by the Company.\n",
    "- **ors_response_id:** Response Identifier for all responses belonging to that company and theme.\n",
    "- **submission_date:** DateTime company response was finalised and submitted to CDP with no further amendments.\n",
    "- **page_name:** Section of the CDP questionnaire the question belongs to.\n",
    "- **module_name:** Module ('Parent Section') of the CDP questionnaire the question belongs to e.g.Questions.\n",
    "- **question_number:** Question number of response.\n",
    "- **question_unique_reference:** Question name.\n",
    "- **colmn_number:** Column number of matrix set (Table) or matrix dynamic (Add Rows Table) column in question  reponse table.\n",
    "- **column_name:** Column name of matrix set (Table) or matrix dynamic (Add Rows Table) column in question  reponse table.\n",
    "- **table_columns_unique_reference:** Column name and number combination modified with '-' seperator from column_name, providing unique column identifer for each question response.\n",
    "- **row_number:** Row number of matrix set (Table) or matrix dynamic (Add Rows Table) row in question  reponse table. If originally submitted in a table format, this will indicate the number of rows of response data has been entered in response to a question.\n",
    "- **row_name:** Row name of matrix set (Table) or matrix dynamic (Add Rows Table) row in question  reponse table. Description of data type for RowNumber where applicable (i.e. Scope 3 emissions category).\n",
    "- **data_point_name:** Question number_Column number_Question Name - Column Name string identifier.\n",
    "- **data_point_id:** Unique identifier for Question Column Response.\n",
    "- **response_value:** Question response submitted by company.\n",
    "- **comments:** Added response clarifications from Company or CDP staff.\n",
    "\n",
    "## 2.4 Dataframe Description\n",
    "- **cid_raw** - Cities Disclosing. Originally imported dataset. Combines ...\n",
    "- **cir_raw** - Cities Disclosing. Originally imported dataset. Combines ...\n",
    "- **cod_raw** - Cities Responses.  Originally imported dataset. Combines ... water and climate\n",
    "- **cor_raw** - Cities Responses.  Originally imported dataset. Combines ... water and climate  \n",
    "\n",
    "\n",
    "- **cir_raw_10** - Cities Disclosing. 10k-reduced originally imported dataset. Combines ...\n",
    "- **cor_raw_10** - Cities Responses.  10k-reduced originally imported dataset. Combines ... water and climate\n",
    "\n",
    "\n",
    "- **cid_ext01** - External Data with AccountNumber, City, Population\n",
    "- **cid_ext02** - External Data with Location\n",
    "\n",
    "\n",
    "- **cid** - Result of DataCleaning: DataFrame for working ...\n",
    "- **cir** - Result of DataCleaning: DataFrame for working ...\n",
    "- **cod** - Result of DataCleaning: DataFrame for working ...\n",
    "- **cor** - Result of DataCleaning: DataFrame for working ...\n",
    "\n",
    "\n",
    "- **cir_10** - Result of DataCleaning: 10k-reduced DataFrame for working ...\n",
    "- **cor_10** - Result of DataCleaning: 10k-reduced DataFrame for working ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Environment Set-Up\n",
    "## 3.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# specific libaries\n",
    "import glob                                     # read all csv files in the directory\n",
    "import os                                       # for using OS functions\n",
    "import warnings                                 # for suppression of depricated messages          \n",
    "import pandas_profiling                         # needs prior install\n",
    "from methods import *                           # selfmade functions and methods\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pygal\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "# ML - PreProcessing\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Global Variables and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')               # for suppression of depricated messages\n",
    "RSEED = 42                                      # for replicability purposes\n",
    "\n",
    "#ipython magic commands\n",
    "%matplotlib inline      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing Response-Datasets for better handling in development-testing\n",
    "def reduce_to_10k(df):\n",
    "    result = resample(df,                       # Dataframe to resample\n",
    "                      replace=False,            # sample without replacement\n",
    "                      n_samples=10000,          # sample size\n",
    "                      random_state=RSEED)       # for reproducible results\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we start by importing the different csv files and continue by concatenating the files into one dataframe.\n",
    "def get_data(path, filename_start):\n",
    "    '''a function to store the content of a directory into a pd dataframe'''\n",
    "    \n",
    "    # checking the contents of the directory using the os-module. \n",
    "    files = [\n",
    "        file for file in os.listdir(path) \n",
    "        if file.startswith(filename_start)\n",
    "        ]\n",
    "    \n",
    "    print(files)  \n",
    "    \n",
    "    # iterate through files and add to the data frame\n",
    "    all_data = pd.DataFrame()\n",
    "    for file in files:\n",
    "        current_data = pd.read_csv(path+\"/\"+file, dtype={'comments': str})\n",
    "        all_data = pd.concat([all_data, current_data], ignore_index=True)\n",
    "\n",
    "    # replace whitespaces from column names \n",
    "    all_data.columns = [i.lower().replace(' ', '_') for i in all_data.columns]\n",
    "        \n",
    "    print(f'''\\nA dataframe with {all_data.shape[0]} rows and {all_data.shape[1]} columns has been created!\\nColumn names are now lower case and spaces are replaced by \"_\".''')\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function comes from Silas and could be an idea for further development.\n",
    "def meta(df, transpose=True):\n",
    "    \"\"\"\n",
    "    This function returns a dataframe that lists:\n",
    "    - column names\n",
    "    - nulls abs\n",
    "    - nulls rel\n",
    "    - dtype\n",
    "    - duplicates\n",
    "    - number of diffrent values (nunique)\n",
    "    \"\"\"\n",
    "    metadata = []\n",
    "    dublicates = sum([])\n",
    "    for elem in df.columns:\n",
    "\n",
    "        # Counting null values and percantage\n",
    "        null = df[elem].isnull().sum()\n",
    "        rel_null = round(null/df.shape[0]*100, 2)\n",
    "\n",
    "        # Defining the data type\n",
    "        dtype = df[elem].dtype\n",
    "\n",
    "        # Check dublicates\n",
    "        duplicates = df[elem].duplicated().any()\n",
    "\n",
    "        # Check number of nunique vales\n",
    "        nuniques = df[elem].nunique()\n",
    "\n",
    "\n",
    "        # Creating a Dict that contains all the metadata for the variable\n",
    "        elem_dict = {\n",
    "            'varname': elem,\n",
    "            'nulls': null,\n",
    "            'percent': rel_null,\n",
    "            'dtype': dtype,\n",
    "            'dup': duplicates,\n",
    "            'nuniques': nuniques\n",
    "        }\n",
    "        metadata.append(elem_dict)\n",
    "\n",
    "    meta = pd.DataFrame(metadata, columns=['varname', 'nulls', 'percent', 'dtype', 'dup', 'nuniques'])\n",
    "    meta.set_index('varname', inplace=True)\n",
    "    meta = meta.sort_values(by=['nulls'], ascending=False)\n",
    "    if transpose:\n",
    "        return meta.transpose()\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_number_cleaning(question_number_string):\n",
    "    dict_l3 = {'a':'1', 'b':'2', 'c':'3', 'd':'4', 'e':'5', \n",
    "               'f':'6', 'g':'7', 'h':'8', 'i':'9', 'j':'10', \n",
    "               'k':'11', 'l':'12', 'm':'13', 'n':'14', 'o':'15', \n",
    "               'p':'16', 'q':'17', 'r':'18', 's':'19', 't':'20', \n",
    "               'u':'21', 'v':'22', 'w':'23', 'x':'24', 'y':'25', 'z':'26'}\n",
    "    last_char = question_number_string[-1]\n",
    "    \n",
    "    if question_number_string == 'Response Language':\n",
    "        q_nr_l1, q_nr_l2, q_nr_l3 = '00','00','01'\n",
    "    elif question_number_string == 'Amendments_question':\n",
    "        q_nr_l1, q_nr_l2, q_nr_l3 = '00','00','02'\n",
    "    elif last_char in  dict_l3:\n",
    "        question_number_string = question_number_string[0:-1]\n",
    "        q_nr_l1 = question_number_string.split('.')[0].zfill(2)\n",
    "        q_nr_l2 = question_number_string.split('.')[1].zfill(2)\n",
    "        q_nr_l3 = dict_l3[last_char].zfill(2)\n",
    "    else:\n",
    "        q_nr_l1 = question_number_string.split('.')[0].zfill(2)\n",
    "        q_nr_l2 = question_number_string.split('.')[1].zfill(2)\n",
    "        q_nr_l3 = '00'\n",
    "    return q_nr_l1, q_nr_l2, q_nr_l3\n",
    "\n",
    "#question_number_cleaning('17.3u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(data, question_number, column_number=[1], row_number=[1], theme='combined',year=[2018,2019,2020]):\n",
    "    '''’A query function that creates a new dataframe with responses from the given data.'''\n",
    "    # Reduktion auf ausgewählte Menge:\n",
    "    responses = data[(data.theme == theme) &\n",
    "                     (data.year.isin(year)) &\n",
    "                     #(data.q_nr == q_nr) &\n",
    "                     (data.question_number == question_number) &\n",
    "                     (data.column_number.isin(column_number)) &\n",
    "                     (data.row_number.isin(row_number)) \n",
    "                    ].copy()\n",
    "\n",
    "    # Ausgabe der Haupt-Frage:\n",
    "    print(f'AnswerCount = {responses.shape[0]}')\n",
    "    #quest_num = data[(data.q_nr == q_nr)].question_number.iat[0]\n",
    "    quest_num = data[(data.question_number == question_number)].question_number.iat[0]\n",
    "    #question = data[(data.q_nr == q_nr)].question_name.iat[0]\n",
    "    question = data[(data.question_number == question_number)].question_name.iat[0]\n",
    "    print(f'QuestionNumber = {quest_num}:\\n{question}')\n",
    "\n",
    "    # Sortierung:\n",
    "    result = responses.sort_values(by=['type',\n",
    "                                       'theme',\n",
    "                                       #'year',\n",
    "                                       'account_number',\n",
    "                                       'response_pnt'])[[#'type',\n",
    "                                                         #'theme',\n",
    "                                                         #'year',\n",
    "                                                         'account_number',\n",
    "                                                         'entity',\n",
    "                                                         'response_pnt',\n",
    "                                                         'column_name',\n",
    "                                                         'row_name',\n",
    "                                                         'response_answer']]\n",
    "    return result\n",
    "\n",
    "#answer_df = get_responses(cir, '040601', [i for i in range(0,8)], [i for i in range(0,26)], theme='combined', year=[2020])\n",
    "#answer_df = get_responses(cir, '1.0a', [1], [i for i in range(1,3)], theme='combined', year=[2020])\n",
    "#answer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_theme(strng):\n",
    "    if strng[0] == 'C':\n",
    "        result = 'climate'\n",
    "    elif strng[0] == 'W':\n",
    "        result = 'water'\n",
    "    else:\n",
    "        result = 'other'\n",
    "    return result\n",
    "\n",
    "#identify_theme('W-EU0.1b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the analysis by loading the given data through our functions and by gathering external sources to account for relevant missing information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Importing from Kaggle (.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cities Disclosing (combining the years 2018, 2019 and 2020)\n",
    "path = 'data/Cities/Disclosing/'\n",
    "filename_start = '20'\n",
    "cid_raw = get_data(path, filename_start)\n",
    "\n",
    "# Saving-to / loading-from pickle:\n",
    "cid_raw.to_pickle(path+'cid_raw.pkl')\n",
    "#cid_raw = pd.read_pickle(path+'cid_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cities Responses (combining the years 2018, 2019 and 2020)\n",
    "path = 'data/Cities/Responses'\n",
    "filename_start = '20'\n",
    "cir_raw = get_data(path, filename_start)\n",
    "\n",
    "# Saving-to / loading-from pickle:\n",
    "cir_raw.to_pickle(path+'cir_raw.pkl')\n",
    "#cir_raw = pd.read_pickle(path+'cir_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corporations Disclosing  (combining Climate_Change and Water_Security with the years 2018, 2019 and 2020)\n",
    "path = 'data/Corporations/Disclosing/'\n",
    "filename_start = '20'\n",
    "cod_raw = get_data(path, filename_start)\n",
    "\n",
    "# Saving-to / loading-from pickle:\n",
    "cod_raw.to_pickle(path+'cod_raw.pkl')\n",
    "#cod_raw = pd.read_pickle(path+'cod_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Corporations Responses  (combining Climate_Change and Water_Security with the years 2018, 2019 and 2020)\n",
    "path = 'data/Corporations/Responses/'\n",
    "filename_start = '20'\n",
    "cor_raw = get_data(path, filename_start)\n",
    "\n",
    "# Saving-to / loading-from pickle:\n",
    "cor_raw.to_pickle(path+'cor_raw.pkl')\n",
    "#cor_raw = pd.read_pickle(path+'cor_raw.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Importing from External Sources (.xls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the best possible overview of the city disclosure data set, we import the **missing city names** as well as the **population** for the corresponding organizations. This will allows us to get a slightly better understanding of the underlying data and helps later on to conduct more accurate analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing additional population data and city names, that were missing\n",
    "cid_ext01 = pd.read_excel('data/cities/cities_final.xls')\n",
    "\n",
    "#merging imported data\n",
    "cid_cleaned = pd.merge(left=cid_raw,\n",
    "                       right=cid_ext01[['City', 'Population']],\n",
    "                       left_on=cid_raw['account_number'],\n",
    "                       right_on=cid_ext01['AccountNumber'],\n",
    "                       how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the key_0 column which is generated during the merge step\n",
    "cid_cleaned.drop('key_0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all `City`and `Population`data, it is worthwhile to import the corresponding geo locations to obtain a better geographical understanding of the dataset. For that, we use the `GeoPy`library and pull latitude and longitude data from the city names. The aim of this effort is to transfer the information to the city dataset and plot the response answers on **KeplerGl** maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the coordinates for each of the cities\n",
    "cid_ext02 = pd.read_excel('data/Cities/city_coordinates_data.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge location data to cleaned city disclosure dataframe\n",
    "cid_raw = pd.merge(left=cid_cleaned,\n",
    "               right=cid_ext02[['lat', 'lon']],\n",
    "               left_on=cid_cleaned['account_number'],\n",
    "               right_on=cid_raw['account_number'],\n",
    "               how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Reducing Datasets for Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing Response-Datasets for better handling in development-testing\n",
    "cir_raw_10 = reduce_to_10k(cir_raw)\n",
    "cor_raw_10 = reduce_to_10k(cor_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Cleaning\n",
    "Fix the inconsistencies within the data and handle the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Cities Disclosing (cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame for working (incl. creating some new colums/features):\n",
    "cid = pd.DataFrame()\n",
    "cid['type']                     = ['cid' for i in cid_raw.index]\n",
    "cid['theme']                    = 'combined'\n",
    "cid['year']                     = cid_raw['year_reported_to_cdp']\n",
    "cid['account_number']           = cid_raw['account_number']\n",
    "cid['public']                   = cid_raw['access']\n",
    "cid['entity']                   = cid_raw['city']           # muss noch geändert werden (cid_ext01)\n",
    "cid['country']                  = cid_raw['country']\n",
    "cid['region']                   = cid_raw['cdp_region']\n",
    "cid['population']               = cid_raw['Population']     # muss noch geändert werden (cid_ext01)\n",
    "cid['city']                     = cid_raw['City']\n",
    "cid['lat']                      = cid_raw['lat']\n",
    "cid['lon']                      = cid_raw['lon']# muss noch geändert werden (cid_ext02)\n",
    "\n",
    "# Due to redundant or missing information the following columns are no longer needed:\n",
    "#cid['organization']            = cid_raw['organization']\n",
    "#cid['reporting_authority']     = cid_raw['reporting_authority']\n",
    "#cid['first_time_discloser']    = cid_raw['first_time_discloser']\n",
    "#cid['population_year']         = cid_raw['population_year']\n",
    "#cid['last_update']             = cid_raw['last_update']\n",
    "#cid['altitude']                = cid_raw['altitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding some features: entity (city), population, city_location\n",
    "\n",
    "# Hier werden dann die Daten von Felix (cid_ext01) und Tobi (cid_ext02) anmontiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading of the full dataset:\n",
    "cid.to_pickle(\"data/Cities/Disclosing/cid.pkl\")\n",
    "#cid = pd.read_pickle(\"data/Cities/Disclosing/cid.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Cities Responses (cir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we will use the geographical information that we retrieved to complete the city disclosure data set and transfer this data to the cities responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer city, latitude, longitude and population data to city response dataframe\n",
    "cir_raw = pd.merge(left=cir_raw,\n",
    "               right=cid[[\"city\", \"lat\", \"lon\", \"population\"]], \n",
    "               left_on=cir_raw[\"account_number\"], \n",
    "               right_on=cid[\"account_number\"], \n",
    "               how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cir_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we again simplify the dataframe for the cities responses by omitting all columns that we perceive as less relevant for the further analysis. In addition, we reduce complexity of the question names by splitting the **question name** into separate columns. This allows for easier access to specific questions in the analysis later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame for working (incl. creating some new colums/features):\n",
    "cir = pd.DataFrame()\n",
    "cir['type']                   = ['cir' for i in cir_raw.index]\n",
    "cir['theme']                  = 'combined'\n",
    "cir['year']                   = cir_raw.apply(lambda x : int(x['questionnaire'][-4:]), axis=1)\n",
    "cir['account_number']         = cir_raw['account_number']\n",
    "cir['entity']                 = cir_raw['organization'] # has to been changed, when cid['city'] is ready\n",
    "cir['country']                = cir_raw['country']\n",
    "cir['region']                 = cir_raw['cdp_region']\n",
    "cir['section']                = cir_raw['section']\n",
    "cir['q_nr_l1']                = cir_raw.apply(lambda x : question_number_cleaning(x['question_number'])[0], axis=1)\n",
    "cir['q_nr_l2']                = cir_raw.apply(lambda x : question_number_cleaning(x['question_number'])[1], axis=1)\n",
    "cir['q_nr_l3']                = cir_raw.apply(lambda x : question_number_cleaning(x['question_number'])[2], axis=1)\n",
    "cir['q_nr']                   = cir['q_nr_l1']+cir['q_nr_l2']+cir['q_nr_l3']\n",
    "cir['question_number']        = cir_raw['question_number']\n",
    "cir['question_name']          = cir_raw['question_name']\n",
    "cir['column_number']          = cir_raw['column_number']\n",
    "cir['column_name']            = cir_raw['column_name']\n",
    "cir['row_number']             = cir_raw['row_number']\n",
    "cir['row_name']               = cir_raw['row_name']\n",
    "cir['response_col']           = cir.apply(lambda x : str(x['year']) +'-'+ x['q_nr'] +'-C'+ str(x['column_number']).zfill(2), axis=1)\n",
    "cir['response_pnt']           = cir.apply(lambda x : str(x['year']) +'-'+ x['q_nr'] +'-C'+ str(x['column_number']).zfill(2) +'-R'+ str(x['row_number']).zfill(2), axis=1)\n",
    "cir['response_answer']        = cir_raw['response_answer']\n",
    "\n",
    "# Due to redundant or missing information the following columns are no longer needed:\n",
    "#cir['questionnaire']         = cir_raw['questionnaire']\n",
    "#cir['year_reported_to_cdp']  = cir_raw['year_reported_to_cdp']\n",
    "#cir['cdp_region']            = cir_raw['cdp_region']\n",
    "#cir['parent_section']        = cir_raw['parent_section']\n",
    "#cir['comments']              = cir_raw['comments']\n",
    "#cir['file_name']             = cir_raw['file_name']\n",
    "#cir['last_update']           = cir_raw['last_update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cir.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das heisst: Es gibt nur 1.141.308 Non-Null Antworten von 1.542.496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting cir[response_answer'] = NaN:\n",
    "print(f'Number of rows before deleting: {cir.shape[0]}')\n",
    "cir.dropna(axis=0, subset=['response_answer'], inplace=True)\n",
    "print(f'Number of rows after deleting:  {cir.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welche verschiedenen Antworten gibt es? Was sind die Favoriten?\n",
    "cir.response_answer.value_counts().sort_values(ascending=False, inplace=False, na_position='first').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Question not applicable' represents about 33% of the answers. \n",
    "# Deleting cir[response_answer'] = 'Question not applicable':\n",
    "print(f'Number of rows before deleting: {cir.shape[0]}')\n",
    "cir = cir.loc[cir['response_answer'] != 'Question not applicable']\n",
    "print(f'Number of rows after deleting:   {cir.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading of the full dataset:\n",
    "cir.to_pickle(\"data/Cities/Responses/cir.pkl\")\n",
    "#cir = pd.read_pickle(\"data/Cities/Responses/cir.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing for better handling in development-testing:\n",
    "cir_10 = reduce_to_10k(cir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Corporations Disclosing (cod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame for working (incl. creating some new colums/features):\n",
    "cod = pd.DataFrame()\n",
    "cod['type']                         = ['cod' for i in cod_raw.index]\n",
    "cod['theme']                        = cod_raw.apply(lambda x : x['questionnaire'].split(' ')[0].lower(), axis=1)\n",
    "cod['year']                         = cod_raw.apply(lambda x : int(x['questionnaire'].split(' ')[-1]), axis=1)\n",
    "cod['account_number']               = cod_raw['account_number']\n",
    "cod['public']                       = cod_raw['public']\n",
    "cod['entity']                       = cod_raw['organization']\n",
    "cod['country']                      = cod_raw['country']\n",
    "cod['addressed_by']                 = cod_raw['samples']\n",
    "cod['minimum_tier']                 = cod_raw['minimum_tier']\n",
    "cod['selected_tier']                = cod_raw['selected_tier']\n",
    "cod['authority_types']              = cod_raw['authority_types']              # String sollte noch bereinigt werden (' Investor, Supply Chain')\n",
    "cod['activities']                   = cod_raw['activities']                   # String mit ausgewählten Branchen/Bereichen. Ansehen!\n",
    "cod['sectors']                      = cod_raw['sectors']                      # String mit ausgewählten Branchen/Bereichen. Ansehen!\n",
    "cod['industries']                   = cod_raw['industries']                   # String mit ausgewählten Branchen/Bereichen. Ansehen!\n",
    "cod['primary_activity']             = cod_raw['primary_activity']             # String mit ausgewählten Branchen/Bereichen. Ansehen!\n",
    "cod['primary_sector']               = cod_raw['primary_sector']               # String mit ausgewählten Branchen/Bereichen. Ansehen!\n",
    "cod['primary_industry']             = cod_raw['primary_industry']             # String mit ausgewählten Branchen/Bereichen. Ansehen!\n",
    "cod['primary_questionnaire_sector'] = cod_raw['primary_questionnaire_sector'] # String mit ausgewählten Branchen/Bereichen. Ansehen!\n",
    "#cod['location']                    = cod_ext03[]                             # muss noch geändert werden (externe Daten)\n",
    "\n",
    "# Due to redundant or missing information the following columns are no longer needed:\n",
    "#cod['survey_year']                 = cod_raw['survey_year']\n",
    "#cod['region']                      = cod_raw['region']\n",
    "#cod['invitation_status']           = cod_raw['invitation_status']\n",
    "#cod['response_received_date']      = cod_raw['response_received_date']\n",
    "#cod['questionnaire']               = cod_raw['questionnaire']\n",
    "#cod['theme']                       = cod_raw['theme']\n",
    "#cod['primary_ticker']              = cod_raw['primary_ticker']\n",
    "#cod['tickers']                     = cod_raw['tickers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cod.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading of the full dataset:\n",
    "cod.to_pickle(\"data/Corporations/Disclosing/cod.pkl\")\n",
    "#cod = pd.read_pickle(\"data/Corporations/Disclosing/cod.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Corporations Responses (cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame for working (incl. creating some new colums/features):\n",
    "cor = pd.DataFrame()\n",
    "cor['type']                     = ['cor' for i in cor_raw.index]\n",
    "cor['theme']                    = cor_raw.apply(lambda x : identify_theme(x['question_number']), axis=1)\n",
    "#cor['theme']                   = [identify_theme(i) for i in cor_raw.question_number]\n",
    "cor['year']                     = cor_raw['survey_year']\n",
    "cor['account_number']           = cor_raw['account_number']\n",
    "#cor['account_number']          = [i for i in cor_raw.account_number]\n",
    "cor['entity']                   = cor_raw['organization']\n",
    "cor['section']                  = cor_raw['module_name']\n",
    "#cor['q_nr_l1']                 =  cor_raw.apply(lambda x : question_number_cleaning(x['question_number'])[0], axis=1)\n",
    "#cor['q_nr_l2']                 = cor_raw.apply(lambda x : question_number_cleaning(x['question_number'])[1], axis=1)\n",
    "#cor['q_nr_l3']                 = cor_raw.apply(lambda x : question_number_cleaning(x['question_number'])[2], axis=1)\n",
    "#cor['q_nr']                    = cor['q_nr_l1']+cor['q_nr_l2']+cor['q_nr_l3']\n",
    "cor['question_number']          = cor_raw['question_number']\n",
    "cor['question_name']            = cor_raw['question_unique_reference']\n",
    "cor['column_number']            = cor_raw['column_number']\n",
    "cor['column_name']              = cor_raw['table_columns_unique_reference']\n",
    "cor['row_number']               = cor_raw['row_number']\n",
    "cor['row_name']                 = cor_raw['row_name']\n",
    "cor['response_answer']          = cor_raw['response_value']\n",
    "\n",
    "# Due to redundant or missing information the following columns are no longer needed:\n",
    "#cor['response_received_date']  = cor_raw['response_received_date']\n",
    "#cor['accounting_period_to']    = cor_raw['accounting_period_to']\n",
    "#cor['ors_response_id']         = cor_raw['ors_response_id']\n",
    "#cor['submission_date']         = cor_raw['submission_date']\n",
    "#cor['page_name']               = cor_raw['page_name']\n",
    "#cor['column_name']             = cor_raw['column_name']\n",
    "#cor['data_point_name']         = cor_raw['data_point_name']\n",
    "#cor['data_point_id']           = cor_raw['data_point_id']\n",
    "#cor['comments']                = cor_raw['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting cor[response_answer'] = NaN:\n",
    "print(f'Number of rows before deleting: {cor.shape[0]}')\n",
    "cor.dropna(axis=0, subset=['response_answer'], inplace=True)\n",
    "print(f'Number of rows after deleting:  {cor.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Olaf -> Achtung: In cor.column_number sind auch NaN! -> Muss erledigt werden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welche verschiedenen Antworten gibt es? Gibt es hier auch so viele 'Question not applicable'?\n",
    "cor.response_answer.value_counts().sort_values(ascending=False, inplace=False, na_position='first').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading of the full dataset:\n",
    "cor.to_pickle(\"data/Corporations/Responses/cor.pkl\")\n",
    "#cor = pd.read_pickle(\"data/Corporations/Responses/cor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing for better handling in development-testing:\n",
    "cor_10 = reduce_to_10k(cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Exploration\n",
    "Form hypotheses about your defined problem by visually analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Cities Disclosing (cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Cities Responding (cir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Corporations Disclosing (cod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Corporations Responses (cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Exploration - Visuals\n",
    "Form hypotheses about your defined problem by visually analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 City Disclosure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our visual data exploration by examining where the disclosing cities are located and visualize their size by population. Looking at the size distribution of cities is important as it will help us to put their **CO2 emissions** into perspective later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize disclosure data using lat and lon data \n",
    "fig = px.scatter_geo(cid, \n",
    "                     lat=\"lat\",\n",
    "                     lon=\"lon\",\n",
    "                     color=\"country\",\n",
    "                     hover_name=\"country\",\n",
    "                     size=\"population\",\n",
    "                     animation_frame=\"year\", # consider replacing thsi with organization count per country\n",
    "                     projection=\"robinson\",\n",
    "                     size_max=100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot survey respondents per country\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.countplot(x=\"region\",palette=\"viridis\", data=cid, order=cid[\"region\"].value_counts().index)\n",
    "plt.xticks(rotation=70)\n",
    "plt.title(\"Survey Participation Distribution per Region\",{'fontsize': 12});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive geographical plots with Keplergl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KeplerGl** allows to interactively explore location data, which we use to analyze the regional distribution of participating cities. From the visualization, we can derive that a major share of participating cities originate from **North and **South America**. The Asian region in contrast is less represented. This is an important observation since the participating corporates are based in North America. Therefore, also having a large share of city respondents from this area facilitates comparisons between cities and corporates later on. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate new KeplerGl map\n",
    "cidmap = KeplerGl(height=600, width=800)\n",
    "\n",
    "# add disclosure data\n",
    "cidmap.add_data(data=cid, name='disclosure_map')\n",
    "cidmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 City Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, an interactive plot of the response distribution per attribute allows to obtain a better understanding of the data source. To avoid longer run-times and improve the handling of the **KeplerGl** map, we visualize only subsets of the full city response dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract subsample from city response dataframe \n",
    "\n",
    "#initiate new map\n",
    "#cirmap = KeplerGl(height=600, width=800)\n",
    "#\n",
    "##load selected city response data\n",
    "#cirmap.add_data(data=cir, name='responses')\n",
    "#cirmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Corporations Disclosure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize corporate responses per survey theme and year\n",
    "cod_raw[\"questionnaire\"].value_counts().plot(kind='bar')  # Tobi: cod hat jetzt cod[\"year\"] + cod[\"theme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the most frequent sectors participating in the survey\n",
    "prob = cod[\"sectors\"].value_counts()\n",
    "threshold = 100\n",
    "mask = prob > threshold\n",
    "tail_prob = prob.loc[~mask].sum()\n",
    "prob = prob.loc[mask]\n",
    "prob['other'] = tail_prob\n",
    "prob.plot(kind='bar')\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Share of Top 5% sectors from all responses\n",
    "\n",
    "#check quantile distribution of sectors\n",
    "prob = cod[\"sectors\"].value_counts()\n",
    "prob /= prob.sum()\n",
    "\n",
    "#split data into quantiles\n",
    "category_classes = pd.qcut(prob, q=[0, .25, 0.95, 1.], \n",
    "                           labels=['bottom 25%', 'mid 70%', 'top 5%'])\n",
    "prob_groups = prob.groupby(category_classes).sum()\n",
    "prob_groups.plot(kind='bar')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent industries\n",
    "prob = cod[\"industries\"].value_counts()\n",
    "threshold = 140\n",
    "mask = prob > threshold\n",
    "tail_prob = prob.loc[~mask].sum()\n",
    "prob = prob.loc[mask]\n",
    "prob['other'] = tail_prob\n",
    "prob.plot(kind='bar')\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Share of top 5% industries\n",
    "prob = cod[\"industries\"].value_counts()\n",
    "prob /= prob.sum()\n",
    "\n",
    "#split data into quantiles\n",
    "category_classes = pd.qcut(prob, q=[0, .25, 0.95, 1.], \n",
    "                           labels=['bottom 25%', 'mid 70%', 'top 5%'])\n",
    "prob_groups = prob.groupby(category_classes).sum()\n",
    "prob_groups.plot(kind='bar')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Corporations Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a fundamental understanding of the underlying city response survey data, we start by visualizing the number of responses per module name and how the number of responses has changed in each module over time. To avoid imbalances, we take into account that each module has a different number different number of questions which in turn differ in their amount of columns and even rows. \n",
    "\n",
    "We use this graphical representation to illustrate where we organization and companies provide most information and can therefore our further analysis on these seemingly more important areas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### still working on the bubble chart that I showed yesterday. \n",
    "### Will only upload the graph if it is 100% accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Feature Engineering\n",
    "Select important features and construct more meaningful ones using the raw data that you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
